"""
PDFæ–‡æ¡£æ‰¹é‡å¤„ç†ç³»ç»Ÿ - æ ¸å¿ƒå·¥ä½œæµè„šæœ¬

åŠŸèƒ½ç‰¹æ€§:
1. åŸºäºPrefectçš„åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦
2. æ”¯æŒå¤šçº§ç›®å½•ç»“æ„è‡ªåŠ¨å¤„ç†
3. é›†æˆPDFè§£æä¸Markdownç”Ÿæˆ
4. å®Œå–„çš„é”™è¯¯å¤„ç†ä¸æ—¥å¿—è¿½è¸ª
5. å¹¶å‘æ§åˆ¶ä¸èµ„æºç®¡ç†
"""

# ------------------------ æ ‡å‡†åº“å¯¼å…¥ ------------------------
from pathlib import Path
from datetime import datetime
import re
import shutil
from typing import Dict
import sys

# ------------------------ ç¬¬ä¸‰æ–¹åº“å¯¼å…¥ ------------------------
import requests
from prefect import flow, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
# ------------------------ ç¼–ç å¼ºåˆ¶è®¾ç½® ------------------------
import sys
import os

# å¯ç”¨Python UTF-8æ¨¡å¼ï¼ˆå…¼å®¹æ€§æœ€ä½³æ–¹æ¡ˆï¼‰
os.environ["PYTHONUTF8"] = "1"
os.environ["PYTHONIOENCODING"] = "utf-8"

# é‡è½½æ ‡å‡†è¾“å‡ºæµçš„ç¼–ç é…ç½®
sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')
# ------------------------ æœ¬åœ°æ¨¡å—å¯¼å…¥ ------------------------
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„ï¼ˆå…¼å®¹æµ‹è¯•ç¯å¢ƒï¼‰
current_dir = Path(__file__).parent
src_dir = current_dir.parent if current_dir.name == 'flows' else current_dir
sys.path.append(str(src_dir))

from utils.str_utils import optimize_str
from tasks.doc_task.base_task import *
from tasks.doc_task.process_pdf_task import process_pdf_file
from utils.file_utils import ensure_directory
from flows.embed_vectorstorage_flow import process_and_store_directory
from flows.test_flow import my_flow

# ------------------------ å…¨å±€é…ç½® ------------------------
DEFAULT_INPUT_DIR = Path("data/raw/pdf")       # é»˜è®¤PDFè¾“å…¥ç›®å½•
DEFAULT_OUTPUT_DIR = Path("data/processed")    # ä¸­é—´æ–‡ä»¶è¾“å‡ºç›®å½•
FINAL_OUTPUT_DIR = Path("data/output/markdown")# æœ€ç»ˆMarkdownå­˜å‚¨ç›®å½•
MAX_CONCURRENCY = 4                            # æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°ï¼ˆæ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´ï¼‰
SAFE_MODE = True                               # å®‰å…¨æ¨¡å¼å¼€å…³ï¼ˆé˜²æ­¢è¯¯åˆ æ–‡ä»¶ï¼‰


    # input_dir: str = "../data/raw/pdf",
    # output_root: str = "../data/processed",
    # final_output_dir: str = "../data/output/markdown"


# ------------------------ ä¸»å·¥ä½œæµ ------------------------
@flow(
    name="pdf_to_markdown",
    description="PDFæ‰¹é‡å¤„ç†ä¸»æµç¨‹ï½œå«å¤šçº§ç›®å½•æ”¯æŒä¸æ™ºèƒ½é‡è¯•æœºåˆ¶"
)
def pdf_to_markdown(
    input_dir: str,
    output_root: str,
    final_output_dir: str,
    kb_id: int,
    image_path: str
) -> Dict:
    """
    PDFæ–‡æ¡£å¤„ç†å…¨æµç¨‹æ§åˆ¶å™¨
    
    ä¸»è¦é˜¶æ®µ:
    1. è·¯å¾„è§„èŒƒåŒ–å¤„ç†
    2. ç›®å½•ç»“æ„éªŒè¯ä¸åˆ›å»º
    3. åˆ†å¸ƒå¼æ–‡ä»¶å¤„ç†
    4. ç»“æœåˆ†æä¸æ¸…ç†
    5. å…ƒæ•°æ®æ±‡æ€»æŠ¥å‘Š

    å‚æ•°:
        input_dir: PDFæºæ–‡ä»¶æ ¹ç›®å½•ï¼ˆæ”¯æŒåµŒå¥—å­ç›®å½•ï¼‰
        output_root: ä¸­é—´æ–‡ä»¶è¾“å‡ºæ ¹ç›®å½•
        final_output_dir: æœ€ç»ˆMarkdownå­˜å‚¨è·¯å¾„

    è¿”å›:
        åŒ…å«å¤„ç†å…ƒæ•°æ®çš„å­—å…¸:
        {
            "total": æ€»æ–‡ä»¶æ•°,
            "success": æˆåŠŸæ•°,
            "failed": å¤±è´¥æ•°,
            "start_time": ISOæ ¼å¼å¼€å§‹æ—¶é—´,
            "duration": æ€»è€—æ—¶(ç§’),
            "output_dir": è¾“å‡ºç›®å½•è·¯å¾„,
            "error_logs": [é”™è¯¯ä¿¡æ¯åˆ—è¡¨]
        }
    """
    logger = get_run_logger()
    
    try:
        # ====================== å‰ç½®æ¸…ç†é˜¶æ®µ ======================
        logger.info("ğŸ§¹ åˆå§‹åŒ–ç›®å½•æ¸…ç†...")
        dirs_to_clean = [
            Path(output_root),
            Path(final_output_dir),
            Path(image_path)
        ]
        
        for dir_path in dirs_to_clean:
            if dir_path.exists():
                logger.warning(f"âš ï¸ æ­£åœ¨å¼ºåˆ¶æ¸…ç†ç›®å½•: {dir_path}")
                shutil.rmtree(dir_path, ignore_errors=False)
                dir_path.mkdir(parents=True, exist_ok=True)
                logger.debug(f"âœ… å·²é‡å»ºç›®å½•: {dir_path}")
            else:
                logger.debug(f"â© ç›®å½•ä¸å­˜åœ¨æ— éœ€æ¸…ç†: {dir_path}")

        # ====================== é˜¶æ®µ0ï¼šè·¯å¾„é¢„å¤„ç† ======================
        logger.debug("ğŸ”„ æ­£åœ¨è§„èŒƒåŒ–è·¯å¾„ç»“æ„...")
        input_path, output_path = convert_paths(input_dir, output_root)
        final_output_path = Path(final_output_dir).resolve()
        
        # ====================== é˜¶æ®µ0_1ï¼šç›®å½•éªŒè¯ ======================
        logger.info("ğŸ” æ‰§è¡Œç›®å½•å®Œæ•´æ€§æ£€æŸ¥...")
        validated_dir = validate_input_dir(input_path)
        ensure_directory(output_path)
        ensure_directory(final_output_path)
        
        # ====================== é˜¶æ®µ3ï¼šæ–‡ä»¶æ”¶é›† ======================
        logger.debug("ğŸ“‚ æ‰«æç›®å½•ç»“æ„...")
        # subdirs = get_subdirectories(validated_dir)
        subdirs = [validated_dir]
        pdf_files = collect_all_pdf_files(subdirs)
        logger.info(f"âœ… å‘ç° {len(pdf_files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")
        
        # ====================== é˜¶æ®µ4ï¼šå¹¶è¡Œå¤„ç† ======================
        logger.info("ğŸš€ å¯åŠ¨æ–‡æ¡£å¤„ç†å¼•æ“...")
        processing_results = mineru_process_pdf_flow(
            pdf_files, 
            validated_dir, 
            output_path,
            final_output_path
        )
        
        # ====================== é˜¶æ®µ5ï¼šç»“æœåˆ†æ ======================
        logger.info("ğŸ“Š ç”Ÿæˆå¤„ç†æŠ¥å‘Š...")
        result_stats = analyze_results(processing_results)
        
        # ====================== é˜¶æ®µ6ï¼šèµ„æºæ¸…ç† ======================
        if not SAFE_MODE:
            logger.warning("âš ï¸ å®‰å…¨æ¨¡å¼å·²å…³é—­ï¼Œæ‰§è¡Œæ¸…ç†æ“ä½œ")
            perform_cleanup(output_path)
            
        # ====================== é˜¶æ®µ7ï¼šå¤åˆ¶å›¾ç‰‡ç›®å½•åˆ°æœåŠ¡å™¨ ======================
        logger.info("ğŸ–¼ï¸ å¼€å§‹å¤åˆ¶å›¾ç‰‡ç›®å½•åˆ°æœåŠ¡å™¨...")
        image_subdirs = get_subdirectories(validate_input_dir(output_root))
        copied_dirs = []
        error_logs = []

        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        ensure_directory(Path(image_path))

        for src_dir in image_subdirs:
            dest_dir = Path(image_path) / src_dir.name
            try:
                # è¦†ç›–å¼å¤åˆ¶ç›®å½•ï¼ˆéœ€è¦Python 3.8+ï¼‰
                shutil.copytree(
                    src_dir, 
                    dest_dir,
                    dirs_exist_ok=True
                )
                copied_dirs.append(str(dest_dir))
                logger.debug(f"âœ… æˆåŠŸå¤åˆ¶ç›®å½•: {src_dir.name} -> {dest_dir}")
            except Exception as e:
                error_msg = f"ç›®å½•å¤åˆ¶å¤±è´¥ [{src_dir.name}]: {str(e)}"
                error_logs.append(error_msg)
                logger.error(error_msg, exc_info=True)

        logger.info(f"ğŸ“¦ å®Œæˆç›®å½•å¤åˆ¶ï¼š{len(copied_dirs)}æˆåŠŸ / {len(error_logs)}å¤±è´¥")
        
        image_base_url = f'http://127.0.0.1:9090/static/images/{kb_id}'
        for md_file in final_output_path.rglob('*.md'):
            try:
                replace_image_paths(md_file,image_base_url)
            except Exception as e:
                logger.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥ [{md_file.name}]: {str(e)}", exc_info=True)
      
        # ====================== é˜¶æ®µ8ï¼šåˆ†å—æ–‡æ¡£å¹¶å¾—åˆ°åµŒå…¥æ•°æ®åº“ ======================
        logger.info("ğŸ§  å¯åŠ¨çŸ¥è¯†åº“åµŒå…¥æµç¨‹...")
        try:
            # é…ç½®å‘é‡å­˜å‚¨å‚æ•°
            embed_config = {
                "models": {
                    "name": "bge-m3:latest",
                    "base_url": "http://127.0.0.1:11434"
                },
                "vector_store": {
                    "base_path": "../../server/med_rag_server/vectorstorage",
                    "naming_template": f"kb_{kb_id}_" + "{model_hash}_{doc_hash}"
                }
            }

            # æ‰§è¡ŒåµŒå…¥æµç¨‹
            vector_manager = process_and_store_directory(
                content_source=final_output_path,
                config=embed_config,
                processor_type="header_hybrid",
                processor_params={
                    "chunk_size": 1500,
                    "semantic_threshold": 0.85
                },
                recursive=True
            )

            # ç»“æœå¤„ç†ä¸çŠ¶æ€æ›´æ–°
            if vector_manager and vector_manager.is_ready:
                logger.info(f"ğŸ“š çŸ¥è¯†åº“æ„å»ºæˆåŠŸ â” {vector_manager.get_store_info()}")
                result_stats["vector_store"] = {
                    "base_path": vector_manager.vector_store_path,
                    "model": embed_config["models"]["name"],
                    "doc_count": len(vector_manager.docs)
                }
                process_status = "completed"
                vector_path = vector_manager.vector_store_path  # è·å–å®é™…å­˜å‚¨è·¯å¾„
            else:
                logger.error("çŸ¥è¯†åº“æ„å»ºå¤±è´¥")
                result_stats["vector_store"] = None
                process_status = "failed"
                vector_path = None

            # è°ƒç”¨åŒæ¥å£æ›´æ–°ï¼ˆæ–°å¢éƒ¨åˆ†ï¼‰
            api_base = "http://localhost:9090/api/knowledge-bases"
            headers = {"Content-Type": "application/json"}
            
            try:
                # ç¬¬ä¸€æ­¥ï¼šæ›´æ–°å¤„ç†çŠ¶æ€
                status_response = requests.patch(
                    f"{api_base}/{kb_id}/processing-status",
                    json={"processing_status": process_status},
                    headers=headers,
                    timeout=10
                )
                
                if status_response.status_code != 200:
                    logger.error(f"çŠ¶æ€æ›´æ–°å¤±è´¥: {status_response.text}")

                # ç¬¬äºŒæ­¥ï¼šæˆåŠŸæ—¶æ›´æ–°è·¯å¾„
                if process_status == "completed" and vector_manager.vector_store_path_name:
                    path_response = requests.patch(
                        f"{api_base}/{kb_id}/vector-path",
                        json={"vector_storage_path": vector_manager.vector_store_path_name},
                        headers=headers,
                        timeout=20
                    )
                    
                    if path_response.status_code == 200:
                        logger.info(f"âœ… å‘é‡è·¯å¾„æ›´æ–°æˆåŠŸ: {vector_manager.vector_store_path_name}")
                    else:
                        logger.error(f"è·¯å¾„æ›´æ–°å¤±è´¥: {path_response.text}")

            except Exception as api_error:
                logger.error(f"APIé€šä¿¡å¼‚å¸¸: {str(api_error)}", exc_info=True)

        except Exception as e:
            logger.error(f"å‘é‡å­˜å‚¨åˆ›å»ºå¤±è´¥: {str(e)}", exc_info=True)
            result_stats["vector_store"] = None
            
            # å¼‚å¸¸æƒ…å†µä»…æ›´æ–°çŠ¶æ€
            try:
                requests.patch(
                    f"{api_base}/{kb_id}/processing-status",
                    json={"processingStatus": "failed"},
                    timeout=5
                )
            except Exception as ex:
                logger.error(f"å¼‚å¸¸çŠ¶æ€æ›´æ–°å¤±è´¥: {str(ex)}")

        finally:
            # æœ€ç»ˆèµ„æºæ¸…ç†ï¼ˆå¯é€‰ï¼‰
            pass
        
        
    except Exception as e:
        logger.critical(f"â€¼ï¸ å…³é”®ç³»ç»Ÿæ•…éšœ: {type(e).__name__}", exc_info=True)
        raise RuntimeError("æ‰¹å¤„ç†æµç¨‹å¼‚å¸¸ç»ˆæ­¢") from e

@task
def replace_image_paths(md_file: Path, replace_path: str) -> bool:
    """
    å®‰å…¨æ›¿æ¢å•ä¸ªMarkdownæ–‡ä»¶çš„å›¾ç‰‡è·¯å¾„
    :param md_file: Markdownæ–‡ä»¶è·¯å¾„
    :param replace_path: æ›¿æ¢çš„åŸºç¡€URL
    :return: æ˜¯å¦æˆåŠŸæ›¿æ¢
    """
    logger = get_run_logger()
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(md_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        modified = False
        new_lines = []
        
        for line in content.split('\n'):
            # ä»…å¤„ç†åŒ…å«å›¾ç‰‡çš„è¡Œ
            if line.startswith('![') and 'images_' in line:
                # å®šä½å…³é”®åˆ†éš”ç¬¦
                bracket_pos = line.find('](')
                images_pos = line.find('images_')
                
                if bracket_pos != -1 and images_pos > bracket_pos:
                    # æå–å›¾ç‰‡æè¿°å’Œæ–‡ä»¶å
                    desc_part = line[:bracket_pos+2]
                    file_part = line[images_pos:]
                    
                    # æ„å»ºæ–°è·¯å¾„ï¼ˆç¡®ä¿URLæ ¼å¼æ­£ç¡®ï¼‰
                    new_url = f"{replace_path.rstrip('/')}/{file_part}"
                    new_line = f"{desc_part}{new_url}"
                    
                    new_lines.append(new_line)
                    modified = True
                    logger.debug(f"æ›¿æ¢æˆåŠŸ | æ–‡ä»¶: {md_file.name}")
                    continue
            
            new_lines.append(line)
        
        # å†™å›ä¿®æ”¹å†…å®¹
        if modified:
            with open(md_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(new_lines))
            return True
        return False
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥ [{md_file.name}]: {str(e)}", exc_info=True)
        return False


# ------------------------ MinerU å¤„ç†PDFçš„æµç¨‹ ------------------------
@flow(name="mineru_process_pdf_flow")
def mineru_process_pdf_flow(
    pdf_file_groups: List[Dict[str, Any]], 
    input_dir: Path, 
    output_root: Path,
    final_output_path: Path
) -> List[Dict]:
    """æ‰¹é‡æ–‡ä»¶å¤„ç†ä»»åŠ¡ï¼ˆç»“æ„åŒ–ç‰ˆæœ¬ï¼‰
    
    å‚æ•°:
        pdf_file_groups: ç»“æ„åŒ–PDFæ–‡ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
            {
                "path": åŸå§‹å­ç›®å½•è·¯å¾„,
                "files": è¯¥ç›®å½•ä¸‹çš„PDFæ–‡ä»¶åˆ—è¡¨
            }
        input_dir: è¾“å…¥æ ¹ç›®å½•
        output_root: ä¸­é—´è¾“å‡ºæ ¹ç›®å½•
        final_output_path: æœ€ç»ˆè¾“å‡ºæ ¹ç›®å½•
        
    è¿”å›:
        å¤„ç†ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
            {
                "original_path": åŸå§‹å­ç›®å½•è·¯å¾„,
                "output_path": è¾“å‡ºå­ç›®å½•è·¯å¾„,
                "final_output_path": æœ€ç»ˆè¾“å‡ºå­ç›®å½•è·¯å¾„,
                "processed_files": å¤„ç†æˆåŠŸçš„æ–‡ä»¶åˆ—è¡¨,
                "failed_files": å¤„ç†å¤±è´¥çš„æ–‡ä»¶åˆ—è¡¨
            }
    """
    logger = get_run_logger()
    try:
        results = []
        for group in pdf_file_groups:
            original_subdir = group["path"]
            files = group["files"]
            
            # è®¡ç®—å¯¹åº”çš„è¾“å‡ºå­ç›®å½•è·¯å¾„
            relative_path = original_subdir.relative_to(input_dir)
            output_subdir = output_root / relative_path
            final_output_subdir = final_output_path / relative_path
            
            processed = []
            failed = []
            
            for pdf in files:
                try:
                    result = process_pdf_file.submit(
                        pdf_file=pdf,
                        input_dir=input_dir,
                        output_root=output_root,
                        final_output_dir=final_output_subdir
                    ).result()
                    processed.append(result)
                except Exception as e:
                    logger.warning(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {pdf} - {str(e)}")
                    failed.append({
                        "file": pdf,
                        "error": str(e)
                    })
            
            results.append({
                "original_path": original_subdir,
                "output_path": output_subdir,
                "final_output_path": final_output_subdir,
                "processed_files": processed,
                "failed_files": failed
            })
            
            logger.info(
                f"å¤„ç†å®Œæˆ - åŸå§‹ç›®å½•: {original_subdir}\n"
                f"ä¸­é—´è¾“å‡º: {output_subdir}\n"
                f"æœ€ç»ˆè¾“å‡º: {final_output_subdir}\n"
                f"æˆåŠŸ: {len(processed)}, å¤±è´¥: {len(failed)}"
            )
        
        return results
    except Exception as e:
        logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")
        raise




# ------------------------ æ‰§è¡Œå…¥å£ ------------------------
if __name__ == "__main__":
    """
    æœ¬åœ°è°ƒè¯•æ¨¡å¼å¯åŠ¨å‘½ä»¤:
    python -m flows.doc_processing \
        --input_dir=data/raw/pdf \
        --output_root=data/processed \
        --final_output_dir=data/output/markdown
    """
    # åˆ›å»ºéƒ¨ç½²å¯¹è±¡

    # éƒ¨ç½²è¿™ä¸ªflow
    pdf_to_markdown.serve(name="pdf_to_markdown-deployment")
    # åŸå§‹Markdownå†…å®¹ç¤ºä¾‹
    # original_content = """![æ–¹ç¨‹ 11-1: å™ªå£°æŒ‡æ•°å› æ•°](E:\MySpeace\med_rag\med-rag-flow\data\processed\test01\images_RevolutionMaximaUserManualCN452-454/efee5da2e6ab47efb1e95387366050513a90bebf5b164318b8b9c7b03864502c.jpg)
    # """
    # final_output_path = Path('../data/output/markdown/test01').resolve()
    # image_base_url = f'http://10.20.92.21/static/123'
    # for md_file in final_output_path.rglob('*.md'):
    #         replace_image_paths(md_file,image_base_url)
    # æ›¿æ¢è·¯å¾„å‰ç¼€
    # new_content = replace_image_paths(original_content, "replace_path")
    # print(new_content)
    # result = pdf_to_markdown(
    #     input_dir="../data/raw/pdf",
    #     output_root="../data/processed",
    #     final_output_dir="../data/output/markdown"
    # )