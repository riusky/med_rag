from collections import Counter, defaultdict
import os
from pathlib import Path
from typing import Any, Dict, List, Tuple
import json
import fitz
from prefect import flow, task, get_run_logger
from utils.str_utils import optimize_str
from magic_pdf.data.dataset import PymuDocDataset
from magic_pdf.config.enums import SupportedPdfParseMethod
from magic_pdf.model.doc_analyze_by_custom_model import doc_analyze
from magic_pdf.data.data_reader_writer import FileBasedDataWriter, FileBasedDataReader
from tasks.llm_task.chat_task import *
from tasks.doc_task.base_task import prepare_output_path

# ------------------------ æ ¸å¿ƒå¤„ç†ä»»åŠ¡ ------------------------
@task(name="process_pdf_file")
def process_pdf_file(
    pdf_file: Path, 
    input_dir: Path, 
    output_root: Path,
    final_output_dir: Path,
) -> Dict:
    """å•ä¸ªæ–‡ä»¶å¤„ç†åŒ…è£…ä»»åŠ¡"""
    logger = get_run_logger()
    try:
        output_dir = prepare_output_path.submit(input_dir, pdf_file, output_root).result()
        result = process_pdf_workflow.submit(
            pdf_file=pdf_file,
            working_dir=output_dir,
            final_output_dir=final_output_dir
        ).result()
        return result
    except Exception as e:
        logger.error(f"æ–‡ä»¶å¤„ç†æµç¨‹å¤±è´¥: {pdf_file.name} - {str(e)}")
        return {
            "status": "failed", 
            "error": str(e),
            "file": str(pdf_file)
        }

@task
def process_pdf_workflow(pdf_file: Path, 
                       working_dir: Path, 
                       final_output_dir: Path) -> Dict:
    """PDFå¤„ç†å®Œæ•´å·¥ä½œæµ"""
    logger = get_run_logger()
    
    try:
        # 1. æå–å†…å®¹
        extract_result = extract_pdf_content.submit(pdf_file, working_dir).result()
        
        # 2. ç”ŸæˆMarkdown
        markdown_result = generate_markdown.submit(pdf_file, extract_result, working_dir, final_output_dir).result()
        
        # 3. å¤åˆ¶åˆ°æœ€ç»ˆä½ç½®
        # copy_result = copy_to_final_location.submit(markdown_result, final_output_dir).result()
        copy_result = True
        
        return {
            "status": "success",
            "working_files": extract_result,
            "markdown_result": markdown_result,
            "copy_result": copy_result
        }
        
    except Exception as e:
        logger.error(f"PDFå¤„ç†å·¥ä½œæµå¤±è´¥: {str(e)}", exc_info=True)
        raise


@task(
    name="extract-pdf-content",
    description="PDFæ–‡æ¡£å†…å®¹æå–ä»»åŠ¡ï½œæå–æ–‡æœ¬/å›¾ç‰‡å¹¶ç”Ÿæˆç»“æ„åŒ–æ•°æ®ã€å¯è§†åŒ–æŠ¥å‘ŠåŠMarkdownæ–‡æ¡£ | ä½¿ç”¨MinerUçš„å®ç°æ–¹å¼ | éœ€è¦æ³¨æ„ä½¿ç”¨ Mineru çš„pythonç¯å¢ƒ",
    tags=["pdf-processing", "data-extraction", "ocr", "MinerU"],
    log_prints=True
)
def extract_pdf_content(pdf_file: Path, output_dir: Path) -> Dict:
    """PDFæ–‡æ¡£å†…å®¹è§£ææµæ°´çº¿ task
    
    æ ¸å¿ƒåŠŸèƒ½:
    - ä½¿ç”¨ MinerU è§£æPDFï¼Œå¾—åˆ°åˆç‰ˆçš„ markdown
    - è‡ªåŠ¨æ£€æµ‹PDFç±»å‹ï¼ˆå¯ç¼–è¾‘æ–‡æœ¬/æ‰«æå›¾åƒï¼‰
    - ç”Ÿæˆç»“æ„åŒ–æ•°æ®(JSON)å’Œå›¾ç‰‡èµ„æº
    - è¾“å‡ºå¯è§†åŒ–åˆ†ææŠ¥å‘Š(PDF)
    - ç”Ÿæˆå¸¦å›¾ç‰‡å¼•ç”¨çš„Markdownæ–‡æ¡£

    å‚æ•°:
        pdf_file (Path): PDFæ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒæœ¬åœ°æˆ–ç½‘ç»œå­˜å‚¨è·¯å¾„
        output_dir (Path): è¾“å‡ºç›®å½•ï¼Œå»ºè®®ä½¿ç”¨ç‹¬ç«‹ç›®å½•é˜²æ­¢æ–‡ä»¶è¦†ç›–

    è¿”å›:
        Dict: åŒ…å«ç”Ÿæˆæ–‡ä»¶å…ƒæ•°æ®çš„å­—å…¸ï¼Œç»“æ„å¦‚ä¸‹:
        {
            "status": "success|error",
            "pdf_stem": "åŸå§‹æ–‡ä»¶å",
            "content_list": "å†…å®¹æ¸…å•jsonè·¯å¾„",
            "middleware_data": "ä¸­é—´æ•°æ®jsonç»“æ„è·¯å¾„",
            "assets_dir": "èµ„æºç›®å½•è·¯å¾„",
            "visualizations": ["å¯è§†åŒ–æŠ¥å‘Šè·¯å¾„1", "è·¯å¾„2"]
        }

    å¼‚å¸¸ç­–ç•¥:
        - é”™è¯¯æ—¥å¿—åŒ…å«å®Œæ•´å †æ ˆè·Ÿè¸ª
    """
    logger = get_run_logger()
    flow_tracker = f"[PDF:{pdf_file.name}]"

    try:
        # é¢„å¤„ç†é˜¶æ®µ
        logger.info(f"ğŸš€ å¯åŠ¨PDFè§£ææµç¨‹ {flow_tracker}")
        logger.debug(f"ğŸ“¥ è¾“å…¥æ–‡ä»¶è·¯å¾„: {pdf_file.resolve()}")
        logger.debug(f"ğŸ“¤ è¾“å‡ºç›®å½•è·¯å¾„: {output_dir.resolve()}")

        pdf_stem = pdf_file.stem
        image_dir = output_dir / f"images_{pdf_stem}"
        logger.info(f"ğŸ› ï¸ åˆ›å»ºèµ„æºç›®å½•: {image_dir.name}")
        image_dir.mkdir(exist_ok=True)
        logger.debug(f"âœ… ç›®å½•åˆ›å»ºéªŒè¯: {image_dir.exists()}")
        
        # åˆå§‹åŒ–è¯»å†™å™¨
        logger.debug("ğŸ”„ åˆå§‹åŒ–æ–‡ä»¶è¯»å†™å™¨...")
        image_writer = FileBasedDataWriter(str(image_dir))
        md_writer = FileBasedDataWriter(str(output_dir))
        logger.info(f"ğŸ“„ è¯»å†™å™¨é…ç½®å®Œæˆ | å›¾ç‰‡ç›®å½•: {image_dir} | æ–‡æ¡£ç›®å½•: {output_dir}")
        
        # æ ¸å¿ƒå¤„ç†æµç¨‹
        logger.info("ğŸ” å¼€å§‹è§£æPDFæ–‡ä»¶ç»“æ„...")
        reader = FileBasedDataReader("")
        pdf_bytes = reader.read(str(pdf_file))
        logger.debug(f"ğŸ“Š PDFå­—èŠ‚å¤§å°: {len(pdf_bytes)/1024:.2f} KB")
        
        ds = PymuDocDataset(pdf_bytes)
        logger.info(f"ğŸ¤– PDFç±»å‹æ£€æµ‹ä¸­...")
        if ds.classify() == SupportedPdfParseMethod.OCR:
            logger.warning("âš ï¸ æ£€æµ‹åˆ°æ‰«æç‰ˆPDFï¼Œå¯ç”¨OCRè¯†åˆ«æ¨¡å¼")
            infer_result = ds.apply(doc_analyze, ocr=True)
            pipe_result = infer_result.pipe_ocr_mode(image_writer)
            logger.debug("ğŸ–¼ï¸ OCRæ¨¡å¼å¤„ç†å®Œæˆï¼Œå›¾ç‰‡æå–æ•°é‡: %d", len(os.listdir(image_dir)))
        else:
            logger.info("âœ… æ£€æµ‹åˆ°å¯ç¼–è¾‘PDFï¼Œä½¿ç”¨æ–‡æœ¬æå–æ¨¡å¼")
            infer_result = ds.apply(doc_analyze, ocr=False)
            pipe_result = infer_result.pipe_txt_mode(image_writer)
            # logger.debug("ğŸ“ æ–‡æœ¬æå–å®Œæˆ",)

        # ç”Ÿæˆä¸­é—´æ–‡ä»¶
        logger.info("ğŸ“¦ ç”Ÿæˆç»“æ„åŒ–æ•°æ®æ–‡ä»¶...")
        content_list_path = output_dir / f"{pdf_stem}_content_list.json"
        pipe_result.dump_content_list(md_writer, content_list_path.name, image_dir.resolve())
        logger.debug(f"ğŸ—‚ï¸ å†…å®¹æ¸…å•æ–‡ä»¶ç”Ÿæˆ: {content_list_path.stat().st_size} bytes")
        
        middle_json_path = output_dir / f"{pdf_stem}_middle.json"
        pipe_result.dump_middle_json(md_writer, middle_json_path.name)
        logger.debug(f"ğŸ“Š ä¸­é—´æ•°æ®æ–‡ä»¶ç”Ÿæˆ: {middle_json_path.stat().st_size} bytes")

        # ç”Ÿæˆå¯è§†åŒ–ç»“æœ
        logger.info("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–åˆ†ææŠ¥å‘Š...")
        model_output = output_dir / f"{pdf_stem}_model.pdf"
        infer_result.draw_model(str(model_output))
        logger.debug(f"ğŸ–¼ï¸ æ¨¡å‹å¯è§†åŒ–æ–‡ä»¶: {model_output.stat().st_size} bytes")
        
        layout_output = output_dir / f"{pdf_stem}_layout.pdf"
        pipe_result.draw_layout(str(layout_output))
        logger.debug(f"ğŸ“ ç‰ˆé¢åˆ†ææ–‡ä»¶: {layout_output.stat().st_size} bytes")

        # ç”ŸæˆMarkdownå†…å®¹
        logger.info("ğŸ“ ç”ŸæˆMarkdownæ–‡æ¡£...")
        md_content = pipe_result.get_markdown(os.path.basename(image_dir))
        md_output = output_dir / f"{pdf_stem}.md"
        with open(md_output, 'w', encoding='utf-8') as f:
            f.write(f"# {pdf_stem}\n\n")
            f.write(md_content)
        logger.debug(f"ğŸ“„ Markdownæ–‡æ¡£ç”Ÿæˆ: {md_output.stat().st_size} chars")

        logger.debug(f"ğŸ‰ æˆåŠŸå®Œæˆå¤„ç† {flow_tracker}")
        return {
            "status": "success",
            "pdf_stem": pdf_stem,
            "content_list_path": str(content_list_path),
            "middle_json_path": str(middle_json_path),
            "image_dir": str(image_dir),
            "visualization_files": [str(model_output), str(layout_output)]
        }
        
    except Exception as e:
        logger.error(f"âŒ ä¸¥é‡é”™è¯¯ {flow_tracker} ç±»å‹: {type(e).__name__}", exc_info=True)
        logger.debug(f"ğŸ›‘ é”™è¯¯å‘ç”Ÿæ—¶çš„ä¸´æ—¶æ–‡ä»¶çŠ¶æ€: {[f.name for f in output_dir.glob('*') if f.is_file()]}")
        raise

@task(name="enhance-markdown-generation", description="Markdownç”Ÿæˆä¼˜åŒ–ä»»åŠ¡ï½œé›†æˆç»“æ„åŒ–æ•°æ®æ¸…æ´—ä¸å¤§çº²æ™ºèƒ½åŒ¹é…ï½œMinerUå¢å¼ºç‰ˆ")
def generate_markdown(
    pdf_file: Path,
    extract_result: Dict,
    output_dir: Path,
    final_output_dir: Path
) -> Dict:
    """ä¸»åè°ƒæµç¨‹"""
    logger = get_run_logger()
    trace_id = f"[{extract_result['pdf_stem']}]"
    
    try:
        # é˜¶æ®µ1ï¼šæ•°æ®åŠ è½½
        raw_data, content_list_path = load_initial_data.submit(extract_result).result()
        
        # é˜¶æ®µ2ï¼šæ•°æ®æ¸…æ´—
        cleaned_data = clean_text_data.submit(raw_data).result()
        
        # é˜¶æ®µ3ï¼šå¤§çº²å¤„ç†
        outline_data, outline_index = process_outline.submit(pdf_file, extract_result['pdf_stem']).result()
        
        # æ–°å¢é˜¶æ®µ3.5ï¼šå¤§çº²-æ•°æ®åŒ¹é…
        matched_data = match_outline_to_data.submit(cleaned_data, outline_index, extract_result['pdf_stem']).result()
        
        # é˜¶æ®µ4ï¼šå†…å®¹ç”Ÿæˆ
        markdown_content = generate_markdown_content.submit(matched_data).result()
        
        # é˜¶æ®µ5ï¼šè¾“å‡ºæŒä¹…åŒ–
        output_metadata = persist_output_files(
            extract_result['pdf_stem'],
            matched_data,
            markdown_content,
            output_dir,
            final_output_dir
        )
        
        return {
            "status": "success",
            "pdf_stem": extract_result['pdf_stem'],
            **output_metadata
        }
        
    except Exception as e:
        logger.error(f"âŒ ä¸»æµç¨‹å¤±è´¥ {trace_id}", exc_info=True)
        raise RuntimeError(f"Markdownç”Ÿæˆå¤±è´¥: {str(e)}") from e

@task
def copy_to_final_location(markdown_result: Dict, final_output_dir: Path) -> Dict:
    """å°†Markdownå¤åˆ¶åˆ°æœ€ç»ˆä½ç½®"""
    logger = get_run_logger()
    try:
        markdown_path = Path(markdown_result["markdown_path"])
        pdf_stem = markdown_result["pdf_stem"]
        
        # æ„å»ºç›®æ ‡è·¯å¾„
        target_dir = final_output_dir / markdown_path.parent.name  # ä¿ç•™å­ç›®å½•ç»“æ„
        target_path = target_dir / f"{pdf_stem}.md"
        
        # æ‰§è¡Œå¤åˆ¶
        target_dir.mkdir(parents=True, exist_ok=True)
        import shutil
        shutil.copy(markdown_path, target_path)

        return {
            "status": "success",
            "original_path": str(markdown_path),
            "final_path": str(target_path)
        }
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶å¤åˆ¶å¤±è´¥: {str(e)}", exc_info=True)
        raise
      



@task
def save_output(md_content: str, output_dir: Path, filename: str) -> Path:
    """ä¿å­˜è¾“å‡ºæ–‡ä»¶"""
    logger = get_run_logger()
    try:
        output_dir.mkdir(parents=True, exist_ok=True)
        output_path = output_dir / f"{filename}.md"
        
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(f"# {filename}\n\n")
            f.write(md_content)
        
        logger.info(f"Saved output to: {output_path}")
        return output_path
    except Exception as e:
        logger.error(f"Failed to save output: {str(e)}")
        raise

@task
def extract_outline(pdf_path):
    """æå–PDFå¤§çº²ç»“æ„"""
    doc = fitz.open(pdf_path)
    outline_data = [{
        "level": item[0],
        "title": item[1],
        "page": max(0, item[2] - 1)  # ç¡®ä¿é¡µç éè´Ÿ
    } for item in doc.get_toc()]
    return outline_data




@task(name="match_outline_to_data", description="æ‰§è¡Œå¤§çº²ä¸æ•°æ®åŒ¹é…", tags=["data-matching"])
def match_outline_to_data(
    cleaned_data: List[Dict], 
    outline_index: Dict,
    pdf_stem: str
) -> List[Dict]:
    """æ‰§è¡Œå¤§çº²ä¸æ–‡æœ¬æ•°æ®çš„æ™ºèƒ½åŒ¹é…
    
    Args:
        cleaned_data: æ¸…æ´—åçš„ç»“æ„åŒ–æ•°æ®
        outline_index: å¤§çº²ç´¢å¼•ç»“æ„
        pdf_stem: æ–‡ä»¶æ ‡è¯†
        
    Returns:
        å¸¦æœ‰æ–‡æœ¬å±‚çº§ä¿¡æ¯çš„æ•°æ®
    """
    logger = get_run_logger()
    trace_id = f"[{pdf_stem}]"
    total_matches = 0
    total_items = len(outline_index.get(0, {}).items())
    
    try:
        # æ„å»ºåå‘ç´¢å¼•åŠ é€ŸåŒ¹é…
        text_index = defaultdict(list)
        for idx, item in enumerate(cleaned_data):
            if item.get('type') == 'text':
                clean_text = optimize_str(item.get('text', ''))
                text_index[clean_text].append(idx)
        # å¤„ç†ç©ºæ•°æ®é›†æƒ…å†µ
        if total_items == 0:
            logger.warning(f"âš ï¸ æ£€æµ‹åˆ°ç©ºæ•°æ®é›† | æ— æ³•æ‰§è¡ŒåŒ¹é…æµç¨‹")
            return cleaned_data
          
        for outline in outline_index.get(0, {}).items():  # å‡è®¾ç¬¬ä¸€é¡µä¸ºå¤§çº²åŸºå‡†é¡µ
            norm_title, outline_level = outline
            matched_items = text_index.get(norm_title, [])
                        
            # åº”ç”¨åŒ¹é…ç»“æœ
            for idx in matched_items:
                item = cleaned_data[idx]
                item['text_level'] = outline_level
                total_matches += 1
                
        logger.info(f"ğŸ”— åŒ¹é…å®Œæˆ | æ€»åŒ¹é…: {total_matches}/{total_items} | å‡†ç¡®ç‡: {total_matches/total_items:.1%}")
        return cleaned_data
    except Exception as e:
        logger.error(f"âŒ åŒ¹é…å¤±è´¥ {trace_id}", exc_info=True)
        raise

@task(name="load_initial_data", description="åŠ è½½åˆå§‹æ•°æ®", tags=["data-loading"])
def load_initial_data(extract_result: Dict) -> Tuple[Dict, Path]:
    """å¤„ç†åŸå§‹æ•°æ®åŠ è½½
    
    Args:
        extract_result: æå–é˜¶æ®µå…ƒæ•°æ®
        
    Returns:
        æ¸…æ´—å‰æ•°æ®å’Œå†…å®¹åˆ—è¡¨è·¯å¾„
    """
    logger = get_run_logger()
    trace_id = f"[{extract_result['pdf_stem']}]"
    
    try:
        content_list_path = Path(extract_result["content_list_path"])
        logger.debug(f"ğŸ“‚ åŠ è½½æ•°æ®è·¯å¾„: {content_list_path}")
        
        with open(content_list_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
            
        logger.info(f"ğŸ“¥ æˆåŠŸåŠ è½½ {len(raw_data)} æ¡åŸå§‹æ•°æ®")
        return raw_data, content_list_path
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥ {trace_id}", exc_info=True)
        raise

@task(name="clean_text_data", description="æ‰§è¡Œæ–‡æœ¬æ¸…æ´—", tags=["data-cleaning"])
def clean_text_data(raw_data: List[Dict]) -> List[Dict]:
    """æ‰§è¡Œç»“æ„åŒ–æ•°æ®æ¸…æ´—ä¼˜åŒ–
    
    Args:
        raw_data: åŸå§‹æå–æ•°æ®
        
    Returns:
        æ¸…æ´—åçš„ç»“æ„åŒ–æ•°æ®
    """
    logger = get_run_logger()
    cleaned_data = []
    text_optimization_stats = {"total": 0, "optimized": 0}
    
    for idx, item in enumerate(raw_data):
        if item.get("type") != "text":
            cleaned_data.append(item)
            continue
            
        logger.debug(f"ğŸ” å¤„ç†æ–‡æœ¬æ¡ç›® {idx}")
        original_text = item.get('text', '')
        
        # æ‰§è¡Œæ–‡æœ¬ä¼˜åŒ–
        optimized = optimize_str(original_text)
        text_optimization_stats["total"] += 1
        
        if optimized != original_text:
            text_optimization_stats["optimized"] += 1
            item['text'] = optimized
            logger.debug(f"âœ¨ ä¼˜åŒ–ç”Ÿæ•ˆ: {original_text[:30]}... â†’ {optimized[:30]}...")
            
        # æ¸…ç†å†—ä½™å­—æ®µ
        if 'text_level' in item and item.get('text_level') == 1:
            del item['text_level']
            
        cleaned_data.append(item)
    
    logger.info(f"ğŸ“Š æ–‡æœ¬æ¸…æ´—å®Œæˆ | æ€»å¤„ç†: {text_optimization_stats['total']} | ä¼˜åŒ–ç‡: {text_optimization_stats['optimized']/text_optimization_stats['total']:.1%}")
    return cleaned_data

@task(name="process_outline", description="å¤„ç†æ–‡æ¡£å¤§çº²", tags=["outline-processing"])
def process_outline(pdf_file: Path, pdf_stem: str) -> Tuple[List[Dict], Dict]:
    """æ‰§è¡Œå¤§çº²å¤„ç†ä¸åŒ¹é…
    
    Args:
        pdf_file: PDFæ–‡ä»¶è·¯å¾„
        pdf_stem: æ–‡ä»¶æ ‡è¯†
        
    Returns:
        (å¤„ç†åå¤§çº²æ•°æ®, å¤§çº²ç´¢å¼•ç»“æ„)
    """
    logger = get_run_logger()
    trace_id = f"[{pdf_stem}]"
    
    try:
        # å®é™…åœºæ™¯åº”è°ƒç”¨çœŸå®å¤§çº²æå–
        outline_data = extract_outline.submit(pdf_file)
        outline_data = [
            {
              "level": 1,
              "title": "å‰‚é‡è°ƒæ•´å› æ•° - ä½¿ç”¨ Auto/Smart mA æ—¶çš„å™ªå£°æŒ‡æ•°è°ƒæ•´æ–¹æ³•",
              "page": 0
            },
            {
              "level": 2,
              "title": "ä¾‹å¦‚ï¼Œè‹¥è¦è®¡ç®—åŸºäºå·²é€‰ ASiR-V çº§åˆ«çš„å™ªå£°æŒ‡æ•° (NI) çš„å‰‚é‡å‡å°‘ï¼Œå¯å°†ä¸‹è¡¨ç”¨äºæ ‡å‡†é‡å»ºç®—æ³•ã€‚",
              "page": 0
            },
        ]
        
        # æ„å»ºå¤§çº²ç´¢å¼•
        outline_index = {}
        for outline in outline_data:
            page = outline['page']
            norm_title = optimize_str(outline['title'])
            
            if page not in outline_index:
                outline_index[page] = {}
            outline_index[page][norm_title] = outline['level']
                
        logger.info(f"ğŸ“‘ å¤§çº²å¤„ç†å®Œæˆ | æ¡ç›®: {len(outline_data)} | ç´¢å¼•é¡µæ•°: {len(outline_index)}")
        return outline_data, outline_index
    except Exception as e:
        logger.error(f"âŒ å¤§çº²å¤„ç†å¤±è´¥ {trace_id}", exc_info=True)
        raise

@task(name="generate_markdown", description="ç”ŸæˆMarkdownå†…å®¹", tags=["content-generation"])
def generate_markdown_content(cleaned_data: List[Dict]) -> str:
    """ç”Ÿæˆæœ€ç»ˆMarkdownæ–‡æ¡£
    
    Args:
        cleaned_data: æ¸…æ´—åçš„ç»“æ„åŒ–æ•°æ®
        
    Returns:
        æ ¼å¼åŒ–åçš„Markdownå­—ç¬¦ä¸²
    """
    logger = get_run_logger()
    md_builder = []
    last_page = -1
    
    for idx, item in enumerate(cleaned_data):
        current_page = item.get('page_idx', -1)
        if current_page != last_page:
            md_builder.append(f"\n<!--page-{{{current_page + 1}}}-->\n")
            logger.debug(f"ğŸ“– é¡µé¢åˆ‡æ¢ â†’ P{current_page+1}")
            last_page = current_page
            
        content_type = item.get('type')
        if content_type == "text":
            processed = handle_text(item)
        elif content_type == "table":
            processed = handle_table.submit(item, cleaned_data, idx).result()
        elif content_type == "image":
            processed = handle_image.submit(item, cleaned_data, idx).result()
        elif content_type == "equation":
            processed = handle_equation(item)
        else:
            processed = ""  # é»˜è®¤ç©ºå†…å®¹
        
        md_builder.append(processed)
        logger.debug(f"âœï¸ å†…å®¹å¤„ç† @æ¡ç›®{idx} | ç±»å‹: {content_type} | é•¿åº¦: {len(processed)}")
        
    return "\n".join(md_builder)

@task(name="persist_outputs", description="æŒä¹…åŒ–è¾“å‡ºæ–‡ä»¶", tags=["output-persistence"])
def persist_output_files(
    pdf_stem: str,
    raw_data: List[Dict],
    markdown: str,
    output_dir: Path,
    final_output_dir: Path
) -> Dict:
    """å¤„ç†è¾“å‡ºæ–‡ä»¶æŒä¹…åŒ–
    
    Args:
        pdf_stem: æ–‡ä»¶æ ‡è¯†
        raw_data: åŸå§‹æ•°æ®
        markdown: ç”Ÿæˆçš„MDå†…å®¹
        output_dir: ä¸­é—´ç›®å½•
        final_output_dir: æœ€ç»ˆç›®å½•
        
    Returns:
        è¾“å‡ºæ–‡ä»¶è·¯å¾„é›†åˆ
    """
    logger = get_run_logger()
    trace_id = f"[{pdf_stem}]"
    
    try:
        # æ„å»ºè¾“å‡ºè·¯å¾„
        optimized_json = output_dir / f"{pdf_stem}_optimized.json"
        final_md = final_output_dir / f"{pdf_stem}.md"
        
        # æŒä¹…åŒ–å¤„ç†
        with open(optimized_json, 'w', encoding='utf-8') as f:
            json.dump(raw_data, f, indent=2, ensure_ascii=False)
        final_md.parent.mkdir(parents=True, exist_ok=True)
        with open(final_md, 'w', encoding='utf-8') as f:
            header = f"# {pdf_stem}\n<!-- Source: {pdf_stem} -->\n\n"
            f.write(header + markdown)
            
        logger.info(f"ğŸ’¾ è¾“å‡ºä¿å­˜å®Œæˆ | JSON: {optimized_json} | MD: {final_md}")
        return {
            "original_json": str(optimized_json.with_suffix('.json')),
            "optimized_json": str(optimized_json),
            "markdown_path": str(final_md)
        }
    except Exception as e:
        logger.error(f"âŒ è¾“å‡ºæŒä¹…åŒ–å¤±è´¥ {trace_id}", exc_info=True)
        raise